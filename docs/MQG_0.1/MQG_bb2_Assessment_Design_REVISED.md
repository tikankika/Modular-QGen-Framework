# BUILDING BLOCK 2: ASSESSMENT DESIGN
## Strategic Planning for Question Generation

**Component Type:** Building Block 2 (formerly Component 2)  
**Framework:** Modular Question Generation System  
**Classification:** üî∂üî∑ HYBRID (pedagogical decisions + systematic structure)  
**Primary Input:** Building Block 1 (Content Analysis with validated Learning Objectives)  
**Primary Output:** Comprehensive Assessment Blueprint  
**Purpose:** Transform learning objectives into systematic, theoretically-grounded assessment strategy  
**Estimated Duration:** 1-2 hours

---

## OVERVIEW AND PURPOSE

Building Block 2 represents the critical transition from content understanding to assessment operationalization. This component transforms the validated learning objectives produced in Building Block 1 into a comprehensive assessment blueprint that guides question generation while maintaining constructive alignment throughout the process.

The assessment design process operates at the intersection of pedagogical theory and practical implementation. Drawing on Biggs and Tang's (2011) framework of constructive alignment, this building block ensures that assessment activities directly measure the learning objectives derived from conducted instruction. The process balances teacher expertise in pedagogical decision-making with systematic frameworks that ensure coverage, balance, and technical feasibility.

Unlike Building Block 1's emphasis on understanding what was taught, Building Block 2 focuses on determining how to assess that learning effectively. This component addresses fundamental assessment design questions: What should students be able to demonstrate? How will their understanding be measured? What balance of cognitive levels is appropriate? How will different question types serve distinct assessment purposes?

The hybrid nature of this building block reflects the dual demands of assessment design. Teachers make pedagogical decisions about assessment purpose, cognitive emphasis, and difficulty distribution based on their understanding of student needs and instructional context. Simultaneously, the systematic framework ensures technical compliance with minimum coverage requirements, valid Bloom's taxonomy application, and feasible question type selection given available platforms and grading resources.

---

## THEORETICAL FOUNDATIONS

### Constructive Alignment in Assessment Design

Assessment design in the Modular QGen framework operates from Biggs and Tang's (2011) principle of constructive alignment, which requires explicit connection between intended learning outcomes, teaching activities, and assessment tasks. Building Block 2 operationalizes this principle by creating a systematic mapping from learning objectives (representing what was taught) to question specifications (defining how learning will be assessed).

Constructive alignment serves multiple pedagogical functions in the assessment design process. It ensures that assessment tasks genuinely measure the cognitive levels specified in learning objectives rather than inadvertently testing at lower levels. It makes assessment criteria transparent to both teachers and students by clearly connecting questions to objectives. It guides the development of feedback that references specific learning objectives, helping students understand not just whether answers are correct but how their performance relates to course goals.

The blueprint produced in this building block functions as a constructive alignment contract. Each question specification explicitly states which learning objective it addresses, at what cognitive level, using which assessment format. This explicit mapping enables systematic validation of alignment before question generation begins, preventing the common pitfall of generating numerous questions that fail to measure intended outcomes.

### Bloom's Taxonomy as Assessment Architecture

Bloom's taxonomy (Anderson & Krathwohl, 2001) provides the cognitive architecture for assessment design in Building Block 2. The framework distinguishes six levels of cognitive processing‚ÄîRemember, Understand, Apply, Analyze, Evaluate, and Create‚Äîeach requiring different question formats and assessment approaches.

The assessment design process uses Bloom's taxonomy not merely as a classification scheme but as a strategic planning tool. Distribution decisions across cognitive levels reflect pedagogical priorities about what types of thinking matter most in a given domain. Foundational courses might emphasize Remember and Understand levels to establish conceptual groundwork. Advanced courses might weight heavily toward Analyze and Evaluate levels that require sophisticated reasoning.

Bloom's taxonomy also guides question type selection. Lower cognitive levels (Remember, Understand) align well with selected-response formats like multiple-choice questions that can efficiently assess whether students possess foundational knowledge. Higher cognitive levels (Apply, Analyze, Evaluate) often require constructed-response formats that allow students to demonstrate reasoning processes. Building Block 2 systematically maps question types to cognitive levels, ensuring that assessment format matches cognitive demand.

### Formative and Summative Assessment Functions

Black and Wiliam's (1998) distinction between formative and summative assessment functions fundamentally shapes the assessment design process in Building Block 2. These assessment purposes require different design decisions regarding feedback timing, attempt limits, question difficulty distribution, and point allocation.

Formative assessment serves learning by providing feedback that helps students identify areas requiring additional study. Formative assessments in the Modular QGen framework typically feature immediate feedback after each question, unlimited attempts allowing iterative learning, and difficulty distributions weighted toward accessible questions that build confidence while identifying gaps. The assessment blueprint for formative purposes prioritizes learning support over measurement precision.

Summative assessment measures achievement for evaluation and grading purposes. Summative assessments feature deferred or absent feedback during testing, limited attempts to prevent answer memorization, and balanced difficulty distributions that discriminate across the full range of student performance. The assessment blueprint for summative purposes prioritizes measurement validity and reliability over learning support.

Building Block 2 requires explicit teacher decision about assessment purpose early in the design process because this decision cascades through subsequent choices about question count, difficulty distribution, point allocation, and feedback approach.

### Question Type Affordances and Constraints

Different question formats offer distinct affordances and constraints for assessing learning. Building Block 2 requires understanding these characteristics to make informed decisions about question type distribution.

Selected-response formats (multiple-choice, true-false, matching) offer efficient automated scoring, consistent evaluation across students, and ability to assess large content domains quickly. However, they constrain students to recognize correct answers rather than generate them, potentially underestimating conceptual understanding. These formats work well for assessing foundational knowledge and conceptual understanding but struggle to measure higher-order reasoning.

Constructed-response formats (fill-in-blank, text area, extended text) require students to generate rather than recognize answers, potentially providing more valid measures of understanding. However, they require manual grading for all but the most constrained responses, limiting scalability. These formats excel at assessing application and analysis but demand significant instructor time.

Interactive formats (inline choice, gap match, hotspot) combine some advantages of both approaches by requiring students to construct responses within structured frameworks. Building Block 2 guides strategic selection of question type mixes that balance assessment validity with practical grading constraints.

---

## PROCESS ARCHITECTURE

### Entry Points and Prerequisites

Building Block 2 accepts three possible entry conditions, each representing different workflow patterns in the Modular QGen framework:

**Standard Entry (Workflow A: Content-Driven):** Teachers arrive with completed Building Block 1 content analysis including validated learning objectives. This represents the ideal sequential path where assessment design builds directly on content understanding. The learning objectives provide the foundation for all subsequent design decisions.

**Standards-Based Entry (Workflow B: Standards-Driven):** Teachers arrive with predetermined learning objectives from curriculum standards (such as Swedish GY25 requirements) rather than derived from content analysis. In this case, Building Block 2 proceeds with provided objectives but may recommend parallel content analysis to validate that objectives align with actual instruction rather than only planned curriculum.

**Revision Entry (Workflow D: Revision-Driven):** Teachers arrive seeking to improve existing assessments. Building Block 2 begins with gap analysis‚Äîsystematically comparing existing questions against learning objectives to identify coverage gaps, alignment issues, and distribution imbalances. The process then proceeds to blueprint development addressing identified gaps.

Regardless of entry point, Building Block 2 requires validated learning objectives as its foundation. Without clear, measurable objectives mapped to appropriate cognitive levels, systematic assessment design cannot proceed.

### Process Overview: Seven Connected Stages

Building Block 2 follows a structured seven-stage process that transforms learning objectives into a complete assessment blueprint. Each stage involves dialogue between teacher and AI system, with the teacher making pedagogical decisions and the AI system providing analysis, recommendations, and calculations based on those decisions.

The stages follow a logical progression from broad strategic decisions to specific operational details:

1. **Learning Objective Validation** ensures objectives are assessable and complete
2. **Assessment Strategy Definition** establishes purpose and constraints
3. **Question Target Determination** sets overall scope
4. **Bloom's Distribution Planning** allocates cognitive emphasis
5. **Question Type Mix Planning** selects appropriate formats
6. **Difficulty Distribution Planning** calibrates challenge levels
7. **Blueprint Construction** synthesizes all decisions into operational specifications

This staged approach prevents premature commitment to specific details before foundational decisions are made. Each stage builds on previous decisions while remaining open to revision if later stages reveal problems with earlier choices.

---

## STAGE 1: LEARNING OBJECTIVE VALIDATION

### Purpose and Process

Stage 1 validates that learning objectives from Building Block 1 are suitable for guiding assessment development. While Building Block 1 produces objectives grounded in content analysis, not all such objectives prove immediately assessable without refinement.

**For Standard Entry (post-Building Block 1):** The AI system presents the validated objectives from Building Block 1 organized by Bloom's level, showing the distribution across cognitive levels. The teacher reviews this distribution to ensure it reflects desired assessment priorities. If Building Block 1 emphasized certain content areas that the teacher now recognizes should receive different weight in assessment, objectives may be adjusted or reprioritized at this stage.

**For Standards-Based Entry:** The AI system receives predetermined objectives from curriculum standards and performs two validation checks. First, it verifies that objectives use action verbs consistent with claimed Bloom's levels (e.g., "explain" indicates Understand level, not Remember). Second, it checks for gaps in cognitive level distribution that might indicate missing important learning goals. The teacher reviews these validations and may refine objective wording or Bloom's classifications.

**For Revision Entry:** The AI system analyzes existing assessment questions to infer the implicit learning objectives they address, then presents these inferred objectives for teacher validation. This reverse-engineering process often reveals that existing assessments inadvertently omit important objectives or test at unintended cognitive levels. Teachers may discover that questions claimed to assess "understanding" actually only require recognition memory, indicating need for objective refinement.

### Validation Criteria

Effective learning objectives for assessment purposes must meet several criteria derived from assessment design literature:

**Observable and Measurable:** Each objective must specify observable student performance that can be measured through questions. Objectives using vague language ("appreciate," "be aware of") require refinement to specify concrete demonstrations of learning. For example, "appreciate the complexity of evolutionary processes" becomes "analyze how multiple factors interact in evolutionary scenarios."

**Appropriate Cognitive Level:** The action verb in each objective must accurately reflect the intended cognitive demand. Common problems include objectives claiming higher cognitive levels while describing lower-level tasks, or objectives grouping multiple cognitive levels together inconsistently. Stage 1 validation ensures each objective clearly targets a single Bloom's level.

**Comprehensive Coverage:** The complete set of objectives must cover all essential content identified in Building Block 1 as Tier 1 (essential) and most content identified as Tier 2 (important). Gaps in objective coverage at this stage create systematic gaps in resulting assessments. The AI system checks that all major concepts from content analysis appear in at least one learning objective.

**Achievable Scope:** Each objective must be specific enough to guide focused question development while broad enough to accommodate multiple questions at different difficulty levels. Overly narrow objectives ("list the three types of natural selection") constrain question variety. Overly broad objectives ("understand evolution") provide insufficient guidance for question development.

### Dialogue Pattern: Validation Discussion

The Stage 1 dialogue follows a structured pattern that surfaces potential problems with objectives before proceeding to assessment design:

```
AI System Initial Presentation:

"Based on [Building Block 1 / provided standards / existing assessment], 
I have organized learning objectives across Bloom's taxonomy levels:

REMEMBER (2 objectives, 18%):
- LO1: [action verb] [content]
- LO2: [action verb] [content]

UNDERSTAND (4 objectives, 36%):
- LO3: [action verb] [content]
- LO4: [action verb] [content]
- LO5: [action verb] [content]
- LO6: [action verb] [content]

APPLY (2 objectives, 18%):
- LO7: [action verb] [content]
- LO8: [action verb] [content]

ANALYZE (2 objectives, 18%):
- LO9: [action verb] [content]
- LO10: [action verb] [content]

EVALUATE (1 objective, 9%):
- LO11: [action verb] [content]

This distribution allocates 54% to foundational levels (Remember, Understand) 
and 45% to higher-order levels (Apply, Analyze, Evaluate).

Coverage check against Building Block 1 content tiers:
‚úÖ All Tier 1 (essential) content represented
‚úÖ Most Tier 2 (important) content represented
‚ö†Ô∏è  Tier 2 concept [X] has no corresponding objective

Questions for validation:
1. Does this distribution reflect your assessment priorities?
2. Should we adjust the balance between foundational and higher-order objectives?
3. Should we add an objective for [missing Tier 2 concept]?
4. Do any objectives need refinement in scope or cognitive level?"
```

Teachers respond by confirming the distribution or requesting modifications. Common modifications include:

**Cognitive Redistribution:** "I want more emphasis on application. Can we adjust to 40% foundational and 60% higher-order?" The AI system explains implications (fewer questions at Remember/Understand levels, more scenarios required) and adjusts objective distribution accordingly.

**Objective Addition:** "Yes, we should add an objective about [missing concept]." The AI system proposes an objective with appropriate action verb and Bloom's level, then asks for validation before incorporating it.

**Objective Refinement:** "LO5 seems too broad. Can we split it into two more specific objectives?" The AI system proposes a refinement that maintains the same content coverage while providing clearer guidance for question development.

**Coverage Validation:** "Why wasn't [concept from Building Block 1] included?" The AI system explains whether the concept appeared in Tier 3 (enrichment, not assessed) or was inadvertently missed, then proposes addition if needed.

### Stage 1 Checkpoint

Stage 1 concludes when the teacher confirms that learning objectives accurately represent what should be assessed, at appropriate cognitive levels, with comprehensive coverage of essential content. The AI system documents this confirmation:

```
Learning Objective Validation: COMPLETE

Final objective set:
- [N] total objectives across [N] cognitive levels
- [N] Remember objectives (X%)
- [N] Understand objectives (X%)
- [N] Apply objectives (X%)
- [N] Analyze objectives (X%)
- [N] Evaluate objectives (X%)

Coverage validation:
‚úÖ All Tier 1 (essential) content represented in objectives
‚úÖ [X%] of Tier 2 (important) content represented in objectives
‚úÖ Cognitive distribution reflects stated assessment priorities

These objectives will guide all subsequent assessment design decisions.

Ready to proceed to Stage 2: Assessment Strategy Definition?
```

Only after explicit teacher confirmation does the process advance to Stage 2.

---

## STAGE 2: ASSESSMENT STRATEGY DEFINITION

### Purpose and Scope

Stage 2 establishes the fundamental assessment purpose and practical constraints that shape all subsequent design decisions. This stage addresses Black and Wiliam's (1998) distinction between formative and summative assessment by requiring explicit teacher articulation of assessment goals and usage context.

Assessment strategy decisions cascade through remaining design stages. The choice between formative and summative purposes directly influences appropriate feedback timing (immediate or deferred), suitable attempt limits (unlimited or restricted), optimal difficulty distribution (confidence-building or discriminating), and question point allocation (uniform or variable). Stage 2 makes these cascading implications explicit so teachers understand how strategic choices constrain operational decisions.

### Formative vs. Summative Assessment Purpose

The AI system presents the formative-summative distinction not as a binary choice but as a continuum with clear prototypes at each end and hybrid possibilities between:

```
Assessment Purpose Framework:

FORMATIVE (Practice and Learning):

Primary goal: Help students identify gaps and improve understanding
Usage context: Self-assessment, practice tests, diagnostic checks
Feedback approach: Immediate, explanatory feedback after each question
Attempt policy: Unlimited attempts to support iterative learning
Difficulty balance: More accessible questions to build confidence
Point value: Often ungraded or low-stakes points
Typical features:
- Correct answer shown immediately
- Explanation of why answers are correct/incorrect
- Hints or guidance provided
- Progress tracking emphasized over performance evaluation

When appropriate:
- Students preparing for summative exams
- Early in instructional units (diagnostic)
- When learning support is primary goal
- When stakes are low

SUMMATIVE (Evaluation and Grading):

Primary goal: Measure achievement level for evaluation purposes
Usage context: Final exams, unit tests, graded assessments
Feedback approach: Deferred or absent during test
Attempt policy: Single attempt or very limited attempts
Difficulty balance: Full range to discriminate performance levels
Point value: Significant contribution to course grade
Typical features:
- Answers not revealed during test
- Feedback delayed until after grading
- No hints or assistance
- Accurate measurement emphasized over learning support

When appropriate:
- End-of-unit evaluations
- Final examinations
- When formal grades required
- When comparing student performance needed

HYBRID Approaches:

Some assessments combine elements from both purposes. Common hybrids:
- "Take-home exam": Summative stakes with formative resources
- "Graded practice": Formative format with minor grade weight
- "Test then review": Summative attempt followed by formative feedback

Each hybrid requires explicit decisions about which features to emphasize.
```

The teacher selects assessment purpose from these options, which the AI system then uses to generate appropriate default recommendations for subsequent stages while noting that all recommendations can be adjusted based on specific context.

### Practical Constraints and Context

Beyond assessment purpose, Stage 2 identifies practical constraints that influence feasible assessment designs:

**Time Available:** How much class time or student time is available for assessment? This constraint directly limits question count and type variety. A 30-minute in-class quiz might accommodate 20-25 questions using selected-response formats, but only 3-5 questions if using extended-response formats requiring substantial writing.

**Student Preparation Level:** How well prepared are students for assessment at planned difficulty levels? Students early in their study of a topic need more scaffolding and accessible questions to avoid discouragement. Advanced students benefit from challenging questions that require sophisticated reasoning.

**Platform Capabilities:** What question types does the learning management system support? Technical constraints may preclude certain interactive formats (hotspot, gap match) or limit question metadata tracking. The AI system should note if desired assessment features exceed platform capabilities.

**Grading Resources:** How much instructor time is available for manual grading? This constraint fundamentally shapes appropriate question type distribution. Formative assessments designed for frequent administration typically emphasize auto-graded formats to reduce instructor burden. Summative assessments might justify manual grading time for constructed-response questions that provide richer assessment data.

**Student Population Size:** How many students will take the assessment? Large enrollments favor automated grading to maintain feasibility. Small enrollments might allow more constructed-response questions that provide detailed feedback about reasoning processes.

The AI system documents these constraints explicitly to ensure subsequent design decisions remain realistic:

```
Assessment Context Documentation:

Purpose: [Formative / Summative / Hybrid]
Rationale: [Teacher's explanation]

Practical Constraints:
- Time available: [X] minutes
- Student preparation level: [early/mid/advanced]  in topic
- Platform: [LMS name] with [specific capabilities]
- Grading resources: [limited/moderate/extensive] instructor time
- Enrollment: [N] students

These constraints will guide question count, type mix, and difficulty decisions 
in subsequent stages.

Proceed to determine appropriate question scope for this context?
```

### Stage 2 Checkpoint

Stage 2 concludes with documented assessment strategy that establishes the foundation for all subsequent design decisions. This checkpoint ensures clarity about fundamental purpose before proceeding to operational details:

```
Assessment Strategy: DEFINED

Assessment Type: [Formative / Summative / Hybrid]

Configuration Defaults Based on Purpose:
- Feedback timing: [immediate / deferred / none]
- Attempt policy: [unlimited / limited to N / single attempt]
- Difficulty emphasis: [accessible / balanced / challenging]
- Point approach: [uniform / weighted by difficulty]

Documented Constraints:
- Time available: [X] minutes
- Student level: [early/mid/advanced] in topic
- Platform: [LMS] with [capabilities]
- Grading resources: [limited/moderate/extensive]
- Enrollment: [N] students

These strategic decisions will guide question count, distribution, and type 
selection in subsequent stages. All defaults may be adjusted based on specific 
needs.

Ready to proceed to Stage 3: Question Target Determination?
```

Only with explicit confirmation does Building Block 2 advance to determining specific question quantities and distributions.

---

## STAGE 3: QUESTION TARGET DETERMINATION

### Purpose and Calculation Framework

Stage 3 determines the appropriate total number of questions for the assessment based on learning objectives, time constraints, and assessment purpose established in previous stages. This determination balances competing considerations: sufficient questions to reliably assess each learning objective, feasible question counts given time constraints, and appropriate scope for assessment purpose.

### Coverage-Based Minimum Calculation

The foundation for question target calculation derives from the principle that each learning objective requires multiple questions to reliably assess student achievement. Single-question assessment of an objective provides insufficient evidence to distinguish between genuine understanding and fortunate guessing or narrow memorization.

Assessment design literature suggests minimum ranges for objective coverage:

- **2 questions per objective:** Minimal coverage suitable only for low-stakes formative assessment
- **3-4 questions per objective:** Adequate coverage for most formative assessments
- **4-6 questions per objective:** Robust coverage appropriate for summative assessments
- **6+ questions per objective:** Extensive coverage for high-stakes summative assessments

The AI system calculates coverage-based targets using these ranges:

```
Coverage-Based Question Target Calculation:

Learning objectives: [N] total

Minimum coverage (2 questions/objective):
[N objectives] √ó 2 = [minimum total] questions

Adequate coverage (3-4 questions/objective):  
[N objectives] √ó 3.5 (average) = [adequate total] questions

Robust coverage (4-6 questions/objective):
[N objectives] √ó 5 (average) = [robust total] questions

Recommendation based on [formative/summative] purpose:
[adequate/robust] coverage = approximately [X] questions
```

This calculation provides the starting point for discussion, not a mandate. Teachers may adjust based on time constraints and other factors.

### Time-Based Constraint Validation

Simultaneously with coverage calculations, the AI system validates time feasibility using general timing guidelines for question types:

**Selected-Response Formats:**
- Multiple choice (single answer): 45-60 seconds per question
- Multiple choice (multiple answers): 60-90 seconds per question  
- True/False: 30-45 seconds per question
- Fill-in-blank: 30-60 seconds per question
- Matching: 60-120 seconds per question set

**Constructed-Response Formats:**
- Text area (short answer): 3-5 minutes per question
- Extended text (essay): 15-30 minutes per question

**Interactive Formats:**
- Inline choice: 45-90 seconds per question
- Gap match: 90-180 seconds per question
- Hotspot: 60-120 seconds per question

The AI system uses weighted average timing based on anticipated question type distribution:

```
Time-Based Feasibility Check:

Available time: [X] minutes

Assuming question type distribution:
- [Y]% selected-response (avg 60 seconds) = [Y% √ó total] questions √ó 1 minute
- [Z]% constructed-response (avg 4 minutes) = [Z% √ó total] questions √ó 4 minutes

Implied maximum questions given time:
If primarily selected-response: [X minutes √∑ 1 min] = approximately [N] questions
If balanced mix: [X minutes √∑ 2 min] = approximately [N] questions  
If primarily constructed-response: [X minutes √∑ 4 min] = approximately [N] questions

Coverage-based target of [C] questions appears [feasible / ambitious / infeasible] 
given [X] minutes available.
```

When coverage-based targets exceed time constraints, the AI system presents options: reduce question count below optimal coverage, extend assessment time, or split into multiple assessment sessions.

### Dialogue Pattern: Target Negotiation

Stage 3 dialogue negotiates between pedagogical ideals (adequate coverage) and practical constraints (available time):

```
AI System Recommendation:

"Based on [N] learning objectives, I recommend [X] questions to provide 
[adequate/robust] coverage:

Coverage analysis:
- [N] objectives √ó [3-4 / 4-6] questions = [X] total
- This provides multiple measurements per objective for [reliable/very reliable] 
  assessment

Time feasibility:
- [X] questions using [expected question type mix]
- Estimated completion time: [Y] minutes
- Available time: [Z] minutes
- Assessment: [Feasible / Ambitious / Requires adjustment]

[If time insufficient:]
Options to consider:
1. Reduce to [lower N] questions (minimum 2 per objective)
2. Extend assessment time to [higher Z] minutes
3. Split into multiple assessment sessions
4. Emphasize faster question types (selected-response)

[If time sufficient:]
This scope should comfortably fit within available time with buffer for 
students who work more slowly.

What question target works for your context?"
```

Teachers typically respond in one of several patterns:

**Accept Recommendation:** "Let's go with [X] questions as recommended." Process proceeds using recommended target.

**Adjust for Time:** "We only have [Z] minutes, so let's reduce to [lower N] questions." AI system recalculates coverage per objective and notes which objectives will receive minimum coverage.

**Increase for Thoroughness:** "I want more robust assessment. Let's increase to [higher N] questions." AI system validates time feasibility and notes increased coverage reliability.

**Request Distribution First:** "Before deciding total, can we discuss how questions would distribute across cognitive levels?" Process loops to show anticipated distribution, then returns to target discussion.

### Uneven Objective Distribution Consideration

Not all learning objectives require equal question allocation. Stage 3 addresses whether to distribute questions evenly or weight certain objectives more heavily:

```
Question Distribution Options:

EVEN distribution:
[N] questions √∑ [M] objectives = [N/M] questions per objective
Every objective receives equal coverage regardless of importance

WEIGHTED distribution:
Core objectives receive more questions than secondary objectives
Example distribution:
- Tier 1 objectives (essential): 60% of questions
- Tier 2 objectives (important): 40% of questions

Which approach fits your priorities?
```

Most teachers choose weighted distribution aligned with content tiers from Building Block 1, ensuring essential content receives proportionally more assessment attention than enrichment content.

### Stage 3 Checkpoint

Stage 3 concludes with agreed-upon question target and distribution approach:

```
Question Target: ESTABLISHED

Total questions: [N]

Rationale:
- Provides [X] questions per objective (average)
- Estimated time: [Y] minutes (fits within [Z] minutes available)
- [Even / Weighted] distribution across objectives

Coverage adequacy: [Minimum / Adequate / Robust]

Next stage will determine how these [N] questions distribute across:
- Bloom's cognitive levels (Remember through Evaluate)
- Question types (MC-Single, MC-Multiple, etc.)
- Difficulty levels (Easy, Medium, Hard)

Proceed to Stage 4: Bloom's Distribution Planning?
```

---

## STAGE 4: BLOOM'S DISTRIBUTION PLANNING

### Purpose and Pedagogical Foundations

Stage 4 determines how the total question count distributes across Bloom's cognitive levels (Remember, Understand, Apply, Analyze, Evaluate, and occasionally Create). This distribution decision fundamentally shapes assessment character by establishing cognitive emphasis‚Äîwhat types of thinking the assessment prioritizes.

Distribution decisions draw on several pedagogical principles:

**Alignment with Learning Objectives:** Question distribution should reflect the distribution of learning objectives across cognitive levels established in Stage 1. If 40% of objectives target Understanding level, approximately 40% of questions should assess Understanding unless teachers explicitly decide to weight differently.

**Assessment Purpose Influence:** Formative assessments often weight toward accessible cognitive levels (Remember, Understand) to build student confidence while identifying gaps. Summative assessments typically seek balanced distributions that discriminate across the full performance range, requiring substantial higher-order questions.

**Content Complexity:** Some content domains naturally emphasize particular cognitive levels. Procedural content may weight toward Apply level (demonstrate procedure execution). Conceptual content may emphasize Understand (explain relationships). Policy analysis may weight toward Evaluate (assess alternatives). Distribution should respect natural cognitive demands of content.

**Student Development Level:** Introductory courses typically emphasize foundational levels while advanced courses emphasize higher-order thinking. Distribution should match where students are in their learning progression.

### Distribution Calculation and Recommendation

The AI system generates initial distribution recommendations using a two-step process:

**Step 1: Objective-Based Default Distribution**

Calculate distribution that mirrors learning objective distribution from Stage 1:

```
Objective-Based Distribution:

Learning Objectives by Level (from Stage 1):
- Remember: [N1] objectives ([X1]%)
- Understand: [N2] objectives ([X2]%)
- Apply: [N3] objectives ([X3]%)
- Analyze: [N4] objectives ([X4]%)
- Evaluate: [N5] objectives ([X5]%)

Proportional Question Distribution:
Total questions: [Q]

- Remember: [Q √ó X1%] = [R] questions
- Understand: [Q √ó X2%] = [U] questions  
- Apply: [Q √ó X3%] = [A] questions
- Analyze: [Q √ó X4%] = [A2] questions
- Evaluate: [Q √ó X5%] = [E] questions

This distribution maintains alignment between objectives and questions.
```

**Step 2: Purpose-Based Adjustment Recommendations**

Adjust default distribution based on formative vs. summative purpose:

```
Purpose-Based Adjustment Recommendations:

[For Formative Assessment:]

Current distribution has [X]% at Remember/Understand and [Y]% at higher levels.

Formative recommendation: Increase accessible questions slightly
- Remember: [R + adjustment] questions ([new %])
- Understand: [U + adjustment] questions ([new %])
- Apply/Analyze/Evaluate: [adjusted] questions ([new %])

Rationale: Formative assessments benefit from more accessible questions that 
help students identify specific gaps without excessive difficulty that 
discourages practice.

[For Summative Assessment:]

Current distribution has [X]% at Remember/Understand and [Y]% at higher levels.

Summative recommendation: Ensure balanced representation across range
- Maintain current distribution if already balanced
- OR increase higher-order slightly if heavily weighted toward foundation

Rationale: Summative assessments should discriminate across performance levels, 
requiring substantial higher-order questions while maintaining foundational 
coverage.
```

### Dialogue Pattern: Distribution Negotiation

The Stage 4 dialogue presents calculations and recommendations, then invites teacher refinement based on pedagogical judgment:

```
AI System Presentation:

"Question distribution across Bloom's levels:

Alignment-based distribution (mirrors objectives):
- Remember: [R] questions ([X1]%)
- Understand: [U] questions ([X2]%)
- Apply: [A] questions ([X3]%)
- Analyze: [A2] questions ([X4]%)
- Evaluate: [E] questions ([X5]%)

[Purpose-based] recommendation: [adjustment description]
Adjusted distribution:
- Remember: [R'] questions ([X1']%)
- Understand: [U'] questions ([X2']%)
- Apply: [A'] questions ([X3']%)
- Analyze: [A2'] questions ([X4']%)
- Evaluate: [E'] questions ([X5']%)

Analysis:
- Foundational (Remember + Understand): [%]
- Higher-order (Apply + Analyze + Evaluate): [%]

This distribution [emphasizes foundational / balances foundation and higher-order / 
emphasizes higher-order thinking].

Does this distribution match your assessment priorities? 

Considerations:
- More foundational questions ‚Üí more accessible, good for formative
- More higher-order questions ‚Üí more discriminating, good for summative
- Balance depends on student readiness and course level"
```

Teachers commonly request adjustments in several patterns:

**Increase Foundation:** "Students struggle with basics. Let's increase Remember and Understand to 60% total." AI system recalculates distribution, reducing higher-order proportionally, and notes implications for discriminating advanced performance.

**Increase Higher-Order:** "This is an advanced class. Let's push Apply/Analyze/Evaluate to 70% total." AI system recalculates, reducing foundation proportionally, and notes students must have strong grasp of basics for this to work well.

**Adjust Specific Level:** "I want more Application questions specifically, not just 'higher-order' generally." AI system adjusts Apply level while maintaining relative balance among other levels.

**Validate Against Content:** "Does this distribution adequately cover [specific topic]?" AI system reviews which objectives relate to that topic and confirms appropriate question allocation, adjusting if needed.

### Distribution Validation Checks

After any distribution adjustment, the AI system validates that the distribution remains pedagogically sound:

```
Distribution Validation:

‚úÖ All Bloom's levels present (avoid complete absence of any level)
‚úÖ Each objective's level has sufficient questions to assess it
‚úÖ Distribution aligns with [formative/summative] purpose
‚úÖ Foundation adequate to assess basic understanding
‚úÖ Higher-order adequate to discriminate performance

[If any validation fails:]
‚ö†Ô∏è  [Description of problem]
Recommended adjustment: [specific fix]
```

Common validation failures include:

- **Insufficient foundation:** Very low Remember/Understand percentages risk assuming prerequisite knowledge not actually established
- **Insufficient higher-order:** Very low Apply/Analyze/Evaluate percentages fail to assess transfer and deep understanding
- **Orphan objectives:** Bloom's level represented in objectives but not in question distribution
- **Coverage mismatch:** Essential objectives (Tier 1) receiving fewer questions than secondary objectives (Tier 2)

### Stage 4 Checkpoint

Stage 4 concludes with finalized Bloom's distribution that will guide question generation:

```
Bloom's Distribution: FINALIZED

Question allocation by cognitive level:
- Remember: [R] questions ([X1]%)
- Understand: [U] questions ([X2]%)
- Apply: [A] questions ([X3]%)
- Analyze: [A2] questions ([X4]%)
- Evaluate: [E] questions ([X5]%)

Distribution character:
- Foundational emphasis: [X]%
- Higher-order emphasis: [Y]%
- Assessment: [Accessible / Balanced / Challenging]

Alignment validation:
‚úÖ Distribution reflects learning objective priorities
‚úÖ Sufficient coverage at each represented cognitive level
‚úÖ Appropriate for [formative/summative] purpose
‚úÖ Feasible for [student level] students

Next stage determines question type mix (selected-response, constructed-response, 
interactive formats) to implement this cognitive distribution effectively.

Proceed to Stage 5: Question Type Mix Planning?
```

---

## STAGE 5: QUESTION TYPE MIX PLANNING

### Purpose and Type Affordances

Stage 5 determines which question format types to use and in what proportions. Question type selection influences both assessment validity (how well questions measure intended learning) and practical feasibility (grading resources, platform capabilities, completion time).

Different question types offer distinct affordances for assessing cognitive levels and content types:

**Selected-Response Formats:**

*Multiple Choice (Single Answer)*: Students select one correct answer from multiple options. This format efficiently assesses recognition of concepts, identification of correct procedures, and selection among alternatives. Works well for Remember (identify term), Understand (select explanation), Apply (choose appropriate procedure), and Analyze (identify relationship) levels. Limited for Evaluate level unless options represent alternative positions requiring judgment.

*Multiple Choice (Multiple Answers)*: Students select all correct answers from options. This format assesses complex understanding requiring recognition that multiple factors contribute to phenomena. Works well for Understand (identify all relevant concepts) and Analyze (identify all applicable principles) levels. More cognitively demanding than single-answer because students must evaluate each option independently.

*True/False*: Students judge statement accuracy. This format enables quick assessment of many factual elements but risks encouraging guessing (50% chance). Works best for Remember (verify facts) and Understand (judge statement accuracy) levels. Often criticized in assessment literature for encouraging superficial engagement, best used sparingly.

*Fill-in-Blank*: Students type word or phrase to complete statement. This format requires recall rather than mere recognition, providing somewhat stronger evidence of learning than multiple choice. Works well for Remember (recall terms), Understand (complete explanation with key term), and Apply (insert appropriate value/formula). Limited to content with clear, constrained correct answers.

*Matching*: Students connect items from two lists (e.g., terms to definitions, concepts to examples). This format efficiently assesses understanding of relationships among multiple related elements. Works well for Remember (match terms to definitions) and Understand (match concepts to examples) levels. Particularly useful when content involves systematic relationships among several elements.

**Constructed-Response Formats:**

*Text Area (Short Answer)*: Students type brief response (typically 1-3 sentences). This format requires students to generate rather than recognize answers, providing stronger evidence of understanding. Works well for Understand (explain briefly), Apply (solve and show work), and Analyze (identify and explain relationship) levels. Requires manual grading but allows partial credit and diagnostic feedback.

*Extended Text (Essay)*: Students compose longer response (multiple paragraphs). This format enables assessment of sophisticated reasoning, synthesis of multiple concepts, and evaluative judgment. Essential for Evaluate level (assess alternatives with justification). Requires substantial manual grading time but provides richest assessment data about student thinking.

**Interactive Formats:**

*Inline Choice*: Students select options embedded within passage text. This format assesses understanding in authentic context, requiring students to complete narrative or procedural description with appropriate terms/concepts. Works well for Understand (complete explanation) and Apply (insert appropriate procedure step) levels.

*Gap Match*: Students drag items into appropriate locations (e.g., labels onto diagram, steps into correct sequence). This format assesses organizational understanding and procedural sequencing. Works well for Understand (organize concept relationships) and Apply (sequence procedure steps) levels.

*Hotspot*: Students click correct regions on image or diagram. This format assesses spatial or visual understanding requiring students to identify features on diagrams. Works well for Remember (locate structures), Understand (identify regions showing phenomena), and Apply (identify where to measure/intervene) levels.

### Distribution Framework: Cognitive-Type Mapping

Stage 5 uses systematic mapping between cognitive levels (from Stage 4 distribution) and appropriate question types:

```
Cognitive Level ‚Üí Suitable Question Types Mapping:

REMEMBER level questions ‚Üí Best served by:
- Primary: Fill-in-blank (recall terms)
- Secondary: Multiple choice (recognize definitions)
- Tertiary: Matching (connect terms to definitions)
- Avoid: Extended text (over-complex for simple recall)

UNDERSTAND level questions ‚Üí Best served by:
- Primary: Multiple choice (select correct explanation)
- Secondary: Short answer (explain briefly)
- Tertiary: Matching (connect concepts to examples)
- Consider: Inline choice (complete explanation)

APPLY level questions ‚Üí Best served by:
- Primary: Multiple choice (select appropriate procedure)
- Secondary: Short answer (solve problem, show work)
- Tertiary: Gap match (sequence procedure steps)
- Consider: Fill-in-blank (insert calculation result)

ANALYZE level questions ‚Üí Best served by:
- Primary: Multiple choice multiple-answer (identify all factors)
- Secondary: Short answer (identify and explain relationship)
- Tertiary: Multiple choice (select relationship description)
- Consider: Hotspot (identify interacting elements)

EVALUATE level questions ‚Üí Best served by:
- Primary: Extended text (assess alternatives with justification)
- Secondary: Multiple choice (select best alternative with complex options)
- Tertiary: Short answer (briefly justify position)
- Limited: Selected-response (difficulty capturing genuine evaluation)
```

This mapping provides default recommendations that teachers may adjust based on context.

### Practical Constraint Integration

Question type distribution must respect practical constraints from Stage 2:

**Grading Resource Constraints:**

Limited grading time ‚Üí Emphasize selected-response formats (auto-gradable)
- Target: 70-90% selected-response
- Constructed-response only for critical objectives requiring it

Moderate grading time ‚Üí Balance selected and constructed-response
- Target: 50-70% selected-response, 30-50% constructed

Extensive grading time ‚Üí Can emphasize constructed-response
- Target: 30-50% selected-response, 50-70% constructed

**Platform Capability Constraints:**

Basic LMS (e.g., early Canvas, Moodle) ‚Üí Limited to standard formats
- Available: Multiple choice, True/False, Fill-blank, Text area
- Unavailable: Most interactive formats (inline choice, gap match, hotspot)

Advanced LMS (e.g., Inspera, recent Canvas) ‚Üí Full format range
- Available: All standard + interactive formats
- May still have limitations on specific features

**Time Constraint Influence:**

Short assessment time ‚Üí Favor faster formats
- Emphasize: Multiple choice, True/False, Fill-blank
- Minimize: Constructed-response (time-consuming)

Adequate assessment time ‚Üí Full format range feasible
- Balance based on cognitive and validity considerations

### Dialogue Pattern: Type Mix Negotiation

The Stage 5 dialogue presents recommended type mix based on constraints, then allows teacher adjustment:

```
AI System Recommendation:

"Question type distribution for [N] questions:

Recommended mix based on:
- Cognitive distribution from Stage 4
- [Grading resource level] from Stage 2  
- [Platform capabilities] from Stage 2
- [Time constraint] from Stage 2

AUTO-GRADED FORMATS ([X]% total):
- Multiple Choice (Single): [N1] questions ([%]) 
  Rationale: Versatile for Remember through Analyze levels
- Multiple Choice (Multiple): [N2] questions ([%])
  Rationale: Complex understanding at Analyze level
- True/False: [N3] questions ([%])
  Rationale: Quick checks for Remember level
- Fill-in-Blank: [N4] questions ([%])
  Rationale: Recall for Remember level
- Matching: [N5] questions ([%])
  Rationale: Relationships for Remember/Understand

MANUAL-GRADED FORMATS ([Y]% total):
- Text Area (Short Answer): [N6] questions ([%])
  Rationale: Brief explanations for Understand/Apply/Analyze
- Extended Text (Essay): [N7] questions ([%])
  Rationale: Required for Evaluate level

[If platform supports interactive formats:]
INTERACTIVE FORMATS ([Z]% total):
- Inline Choice: [N8] questions ([%])
  Rationale: Contextual understanding for Understand/Apply
- Gap Match: [N9] questions ([%])
  Rationale: Sequencing for Apply level
- Hotspot: [N10] questions ([%])
  Rationale: Spatial identification for Remember/Apply

Analysis:
- Auto-graded: [X]% (feasible for [grading constraint])
- Manual-graded: [Y]% (manageable with [grading resource])
- Type variety: [assessment - Good/Excellent variety]

Does this type mix work for your assessment needs?

Adjustments to consider:
- Increase/decrease manual-graded %
- Add/remove specific types
- Emphasize particular formats"
```

Teachers commonly request adjustments:

**Reduce Manual Grading:** "I have limited time. Can we reduce short answer and essay?" AI system redistributes to selected-response while noting loss of assessment depth for higher-order thinking.

**Increase Variety:** "This seems too heavily multiple choice. Can we diversify?" AI system increases matching, fill-blank, and (if available) interactive formats while explaining slight time increase.

**Add Missing Format:** "Can we include [specific type]?" AI system adds requested type if platform supports it, redistributing questions from other types, and notes purpose that format serves.

**Platform Limitation:** "My LMS doesn't support [format]. What alternatives?" AI system proposes substitute formats achieving similar assessment goals using available capabilities.

### Type Distribution Validation

After adjustments, the AI system validates the question type mix:

```
Type Distribution Validation:

‚úÖ All cognitive levels have appropriate question types
‚úÖ Auto-graded % feasible for grading resources
‚úÖ All types supported by platform
‚úÖ Type variety adequate (avoiding over-reliance on single format)
‚úÖ Format matches content characteristics

[If validation issues:]
‚ö†Ô∏è  [Description of problem]
Recommended adjustment: [specific fix]
```

### Stage 5 Checkpoint

Stage 5 concludes with finalized question type distribution:

```
Question Type Mix: FINALIZED

Type distribution for [N] total questions:

Selected-Response (Auto-graded): [X]%
- Multiple Choice (Single): [N1] questions
- Multiple Choice (Multiple): [N2] questions
- True/False: [N3] questions
- Fill-in-Blank: [N4] questions
- Matching: [N5] questions

Constructed-Response (Manual-graded): [Y]%
- Text Area: [N6] questions
- Extended Text: [N7] questions

[If applicable:]
Interactive Formats: [Z]%
- Inline Choice: [N8] questions
- Gap Match: [N9] questions
- Hotspot: [N10] questions

Validation:
‚úÖ Mix appropriate for cognitive distribution
‚úÖ Grading workload: [feasible/manageable] for available resources
‚úÖ All types supported by [platform]
‚úÖ Type variety provides [good/excellent] assessment balance

Next stage determines difficulty distribution (easy, medium, hard) across 
these question types and cognitive levels.

Proceed to Stage 6: Difficulty Distribution Planning?
```

---

## STAGE 6: DIFFICULTY DISTRIBUTION PLANNING

### Purpose and Difficulty Conceptualization

Stage 6 determines how questions distribute across difficulty levels (typically Easy, Medium, Hard). Difficulty decisions influence assessment accessibility, discriminatory power (ability to distinguish performance levels), and student experience. This stage integrates difficulty considerations with previously established cognitive levels and question types to create comprehensive specifications for each question.

Difficulty operates as a distinct dimension from cognitive level. A Remember-level question asking students to recall a rarely mentioned term may be quite difficult due to unfamiliarity, while an Analyze-level question asking students to compare concepts thoroughly discussed in class may be relatively accessible due to extensive prior engagement. Difficulty reflects multiple factors: concept familiarity, contextual complexity, time pressure, required integration of multiple elements, and cognitive load.

### Difficulty Framework and Definitions

The AI system presents standardized difficulty definitions that maintain consistency across questions:

```
Difficulty Level Definitions:

EASY:
- Assesses well-practiced content
- Uses familiar contexts and examples
- Requires single-step reasoning
- Correct answer clearly distinguishable from distractors
- Minimal cognitive load
- Expected success rate: 70-90%

MEDIUM:
- Assesses content requiring integration
- Uses contexts similar to instruction but not identical
- Requires two-step reasoning or simple integration
- Distractors require discrimination but reasonable
- Moderate cognitive load
- Expected success rate: 40-70%

HARD:
- Assesses content requiring synthesis or transfer
- Uses novel contexts requiring adaptation of concepts
- Requires multi-step reasoning or complex integration
- Distractors closely resemble correct answer
- High cognitive load
- Expected success rate: 20-40%

These definitions combine content familiarity, contextual complexity, and 
reasoning demands.
```

### Distribution Patterns by Assessment Purpose

Difficulty distribution varies substantially based on formative vs. summative purpose established in Stage 2:

**Formative Assessment Distribution:**

```
Recommended Difficulty Distribution for Formative Assessment:

EASY: 40-50% of questions
MEDIUM: 35-45% of questions
HARD: 10-20% of questions

Rationale:
Formative assessments serve learning rather than evaluation. Higher proportion 
of accessible questions helps students:
- Build confidence through initial success
- Identify specific gaps (not feel overwhelmed)
- Progress from accessible to challenging
- Maintain engagement rather than become discouraged

Hard questions remain important to challenge advanced students and preview 
summative expectations, but dominating with difficulty discourages practice.

Example distribution for 40 questions:
- 18 Easy (45%)
- 16 Medium (40%)
- 6 Hard (15%)
```

**Summative Assessment Distribution:**

```
Recommended Difficulty Distribution for Summative Assessment:

EASY: 20-30% of questions
MEDIUM: 45-55% of questions
HARD: 20-30% of questions

Rationale:
Summative assessments measure achievement across performance range. Balanced 
distribution helps assessment:
- Discriminate among performance levels effectively
- Avoid floor effects (everyone fails) or ceiling effects (everyone succeeds)
- Maintain validity by ensuring adequate challenge
- Represent full scope of learning expectations

Easy questions establish baseline and prevent complete failure. Medium questions 
differentiate most students. Hard questions identify exceptional performance.

Example distribution for 40 questions:
- 10 Easy (25%)
- 22 Medium (55%)
- 8 Hard (20%)
```

### Cognitive Level and Difficulty Interaction

Difficulty interacts with cognitive level in complex ways. Stage 6 integrates these dimensions:

```
Cognitive Level √ó Difficulty Integration:

REMEMBER level can vary in difficulty:
- Easy: Recall frequently emphasized term
- Medium: Recall term mentioned but not emphasized
- Hard: Recall term mentioned once in complex context

UNDERSTAND level can vary in difficulty:
- Easy: Explain concept thoroughly discussed
- Medium: Explain relationship between concepts
- Hard: Explain concept in novel context requiring adaptation

APPLY level can vary in difficulty:
- Easy: Apply procedure to standard problem
- Medium: Apply procedure to problem with minor variation
- Hard: Adapt procedure to novel problem type

ANALYZE level can vary in difficulty:
- Easy: Compare two concepts directly discussed together
- Medium: Compare concepts discussed separately requiring connection
- Hard: Analyze relationships among multiple concepts requiring synthesis

EVALUATE level can vary in difficulty:
- Easy: Evaluate alternatives using provided criteria
- Medium: Evaluate alternatives developing appropriate criteria
- Hard: Evaluate in complex scenario requiring nuanced judgment

Each cognitive level spans full difficulty range. Stage 6 planning ensures 
appropriate difficulty variety within each level.
```

### Dialogue Pattern: Distribution Negotiation

The Stage 6 dialogue presents recommended difficulty distribution based on assessment purpose, then facilitates teacher adjustment:

```
AI System Recommendation:

"Difficulty distribution for [N] questions:

Based on [formative/summative] purpose, I recommend:

EASY: [N1] questions ([X1]%)
MEDIUM: [N2] questions ([X2]%)
HARD: [N3] questions ([X3]%)

Distribution by cognitive level:

REMEMBER ([R] questions):
- Easy: [Re] questions
- Medium: [Rm] questions  
- Hard: [Rh] questions

UNDERSTAND ([U] questions):
- Easy: [Ue] questions
- Medium: [Um] questions
- Hard: [Uh] questions

[Continues for all cognitive levels]

This distribution [builds confidence / discriminates performance / balances 
accessibility and challenge] appropriate for [formative/summative] purpose.

Expected performance:
- Most students succeed on Easy questions (70-90% correct)
- Students differentiate on Medium questions (40-70% correct)
- Advanced students distinguish themselves on Hard questions (20-40% correct)

Does this difficulty profile match your assessment goals?

Considerations:
- More Easy ‚Üí more accessible, less discriminating
- More Hard ‚Üí more challenging, risks discouragement
- Balanced ‚Üí serves diverse student performance levels"
```

Teachers typically adjust difficulty distribution in several patterns:

**Increase Accessibility:** "Students struggle with this material. Let's increase Easy to 50% and reduce Hard." AI system recalculates, noting improved confidence-building but reduced discrimination among advanced students.

**Increase Challenge:** "This is an honors section. Let's increase Hard to 35% and reduce Easy." AI system recalculates, noting improved discrimination but potential discouragement for struggling students.

**Balance Within Cognitive Level:** "I want easier questions at Remember level specifically." AI system adjusts Remember-level difficulty distribution while maintaining overall targets, explaining rationale.

**Question Grade Weighting:** "Should Hard questions be worth more points?" AI system presents options for uniform vs. weighted point schemes, discussing implications for student strategy and grade distribution.

### Difficulty Assignment Strategy

After finalizing overall distribution, Stage 6 addresses how difficulty levels assign to specific questions:

```
Difficulty Assignment Approaches:

DISTRIBUTED Approach:
Each cognitive level receives proportional difficulty distribution
- Every level has some Easy, Medium, Hard questions
- Maintains variety within each level
- Recommended for most assessments

Example for Remember level (10 questions):
- 4 Easy (40%)
- 4 Medium (40%)
- 2 Hard (20%)

PROGRESSIVE Approach:
Lower cognitive levels emphasize easier difficulty
Higher cognitive levels emphasize harder difficulty
- Aligns cognitive and difficulty progression
- Creates natural scaffolding
- Recommended when cognitive levels build sequentially

Example progression:
- Remember: 60% Easy, 30% Medium, 10% Hard
- Understand: 50% Easy, 40% Medium, 10% Hard
- Apply: 30% Easy, 50% Medium, 20% Hard
- Analyze: 20% Easy, 40% Medium, 40% Hard
- Evaluate: 10% Easy, 30% Medium, 60% Hard

Which approach better fits your assessment design?
```

Most teachers select distributed approach for balanced assessments, progressive approach when cognitive levels represent clear skill progression.

### Stage 6 Checkpoint

Stage 6 concludes with comprehensive difficulty specifications integrated with previous distributions:

```
Difficulty Distribution: FINALIZED

Overall difficulty profile for [N] questions:
- Easy: [N1] questions ([X1]%)
- Medium: [N2] questions ([X2]%)
- Hard: [N3] questions ([X3]%)

Distribution by cognitive level:
[Table showing breakdown of each cognitive level by difficulty]

Assignment approach: [Distributed / Progressive]

Expected performance characteristics:
- Accessibility: [High/Moderate/Low] (based on Easy %)
- Discrimination: [High/Moderate/Low] (based on Medium/Hard balance)
- Challenge level: [Accessible/Balanced/Demanding]

Validation:
‚úÖ Difficulty distribution appropriate for [formative/summative] purpose
‚úÖ Adequate variety within each cognitive level
‚úÖ Expected performance patterns align with assessment goals
‚úÖ Point allocation approach determined

All distribution dimensions now specified:
‚úÖ Question count: [N] questions
‚úÖ Cognitive distribution: Remember through Evaluate
‚úÖ Question type mix: Selected/Constructed/Interactive
‚úÖ Difficulty distribution: Easy/Medium/Hard

Ready to synthesize into comprehensive Assessment Blueprint in Stage 7.

Proceed to Stage 7: Blueprint Construction?
```

---

## STAGE 7: BLUEPRINT CONSTRUCTION AND VALIDATION

### Purpose and Synthesis

Stage 7 synthesizes all decisions from previous stages into a comprehensive Assessment Blueprint document that provides complete specifications for each question. This blueprint functions as the contract between assessment design and question generation‚ÄîBuilding Block 4 will generate questions precisely matching blueprint specifications.

The blueprint serves multiple purposes: operational guidance for question generation, validation tool for coverage and alignment, communication artifact for stakeholders, and reference document during quality assurance. Creating a detailed blueprint before question generation prevents common problems: unintended coverage gaps, misalignment between questions and objectives, imbalanced distributions discovered only after substantial work is complete.

### Blueprint Structure and Components

The Assessment Blueprint organizes information hierarchically, from overall assessment configuration to individual question specifications:

**Level 1: Assessment Configuration**
- Assessment type (formative/summative/hybrid)
- Total questions and points
- Time allocation
- Attempt policy
- Feedback configuration
- Technical platform requirements

**Level 2: Learning Objective Documentation**
- Complete list of objectives organized by Bloom's level
- Each objective with identifier, description, cognitive level
- Connection to content tiers from Building Block 1

**Level 3: Distribution Strategy**
- Bloom's level distribution (counts and percentages)
- Question type distribution (counts and percentages)
- Difficulty distribution (counts and percentages)
- Rationale for each distribution decision

**Level 4: Question Specification Table**
- Individual row for each planned question
- Columns: Question number, Learning objective, Bloom's level, Question type, Difficulty, Points, Content focus
- This table provides precise specifications for Building Block 4

**Level 5: Coverage Validation**
- Matrix showing questions per learning objective
- Verification that all objectives meet minimum coverage (typically 2+ questions)
- Identification of any coverage gaps requiring attention

### Blueprint Generation Process

The AI system generates the initial blueprint by systematically compiling decisions from all previous stages:

```
AI System Blueprint Generation:

"Compiling assessment blueprint from Stages 1-6...

‚úÖ Assessment configuration (Stage 2)
‚úÖ Learning objectives (Stage 1) 
‚úÖ Question target (Stage 3)
‚úÖ Bloom's distribution (Stage 4)
‚úÖ Question type mix (Stage 5)
‚úÖ Difficulty distribution (Stage 6)

Generating Question Specification Table...

Allocating questions to objectives ensuring:
- Minimum 2 questions per objective
- Preferred 3-4 questions per objective where possible
- Proportional allocation matching objective importance

Assigning question types to cognitive levels:
- Remember level ‚Üí [appropriate types]
- Understand level ‚Üí [appropriate types]
[continues]

Distributing difficulty within cognitive levels:
- [Distributed / Progressive] approach applied

Creating complete blueprint document...

Blueprint generated. Validation checks running..."
```

### Automated Validation Checks

Before presenting the blueprint, the AI system runs comprehensive validation checks:

```
Blueprint Validation Checks:

COVERAGE VALIDATION:
‚úÖ All learning objectives included
‚úÖ All objectives have minimum 2 questions
‚úÖ Essential objectives (Tier 1) adequately covered
‚ö†Ô∏è  [If any objective under-covered]

DISTRIBUTION VALIDATION:
‚úÖ Bloom's question counts match Stage 4 targets
‚úÖ Question type counts match Stage 5 targets
‚úÖ Difficulty counts match Stage 6 targets
‚ö†Ô∏è  [If any distribution mismatches targets]

ALIGNMENT VALIDATION:
‚úÖ Question types appropriate for cognitive levels
‚úÖ Difficulty distribution reasonable within cognitive levels
‚úÖ No impossible combinations (e.g., Remember + Hard + Essay)
‚ö†Ô∏è  [If any alignment issues]

TECHNICAL VALIDATION:
‚úÖ All question types supported by platform
‚úÖ Total questions feasible for available time
‚úÖ Manual grading load feasible for resources
‚ö†Ô∏è  [If any technical constraints violated]

QUALITY CHECKS:
‚úÖ Variety adequate (no single type dominates excessively)
‚úÖ Progression logical (difficulty and cognitive levels)
‚úÖ Point allocation appropriate
‚ö†Ô∏è  [If any quality concerns]

[Summary]
Issues found: [N]
Warnings: [N]  
Recommendations: [N]
```

If validation identifies issues, the AI system explains problems and proposes corrections before presenting the blueprint to the teacher.

### Blueprint Presentation and Teacher Review

After validation, the AI system presents the complete blueprint with executive summary:

```
Assessment Blueprint: COMPLETE

Executive Summary:

Assessment Type: [Formative / Summative / Hybrid]
Total Questions: [N]
Total Points: [P]
Estimated Time: [T] minutes

Learning Objectives: [M] objectives across [X] cognitive levels
Coverage: All Tier 1 content, [%] of Tier 2 content

Distribution Profile:
- Cognitive: [%] foundational, [%] higher-order
- Format: [%] auto-graded, [%] manual-graded
- Difficulty: [%] Easy, [%] Medium, [%] Hard

Assessment Character: [Accessible/Balanced/Challenging] [formative/summative] 
assessment with [good/excellent] question variety

Validation Status: [All checks passed / N warnings addressed / Ready for use]

[COMPLETE BLUEPRINT DOCUMENT FOLLOWS]

This blueprint provides complete specifications for question generation in 
Building Block 4. 

Review the complete blueprint below. Approve to proceed?"
```

The complete blueprint document follows in standardized format (detailed in Output Structure section below).

### Teacher Review and Refinement

Teachers review the blueprint systematically, commonly checking several aspects:

**Coverage Review:** "Does objective [LO-X] have enough questions?" AI system shows current allocation and can adjust if needed.

**Balance Review:** "This seems heavy on multiple choice. Can we diversify?" AI system shows type distribution, discusses if it aligns with Stage 5 decisions or represents revision.

**Difficulty Review:** "Are there any Easy questions for [cognitive level]?" AI system shows difficulty distribution within that level and can adjust if imbalanced.

**Content Focus Review:** "Question [Q-X] focuses on [topic]‚Äîis that right for that objective?" AI system reviews mapping between question focus and objective, clarifies or adjusts if misaligned.

**Sequencing Review:** "Should questions be ordered by difficulty or by topic?" AI system discusses sequencing options (grouped by objective, progressive difficulty, mixed) and teacher preference.

If review identifies needed changes, the AI system updates the blueprint and re-runs validation checks to ensure consistency.

### Blueprint Finalization Protocol

The blueprint finalizes only after explicit teacher approval:

```
Blueprint Approval Protocol:

Teacher confirmation required:

1. Coverage verification:
   "All [M] learning objectives adequately covered? [Yes/No]"

2. Distribution verification:
   "Bloom's, type, and difficulty distributions appropriate? [Yes/No]"

3. Alignment verification:
   "Question specifications align with learning objectives? [Yes/No]"

4. Feasibility verification:
   "Assessment feasible given time and grading resources? [Yes/No]"

5. Quality verification:
   "Overall blueprint meets assessment quality standards? [Yes/No]"

If all verifications confirmed, blueprint status changes to FINALIZED.

Teacher approval signature: [Date/Time]
Blueprint locked. Changes require formal revision process.

This blueprint now serves as authoritative specification for question generation.
```

Once finalized, the blueprint becomes the contract for Building Block 4. Significant changes require formal revision rather than ad-hoc adjustment to maintain systematic integrity.

### Stage 7 Checkpoint

Stage 7 concludes with approved, validated blueprint ready for question generation:

```
Assessment Blueprint: FINALIZED AND APPROVED

Blueprint Status:
‚úÖ All 7 stages completed
‚úÖ All validation checks passed
‚úÖ Teacher approval received
‚úÖ Blueprint locked for consistency

Blueprint provides complete specifications:
- [N] questions fully specified
- All [M] objectives covered
- Distributions validated and approved
- Technical requirements documented

Deliverable: [Topic]_Assessment_Blueprint.md

Next steps:

OPTION A - Proceed to Building Block 3 (Technical Setup):
If metadata and platform configuration needed before question generation

OPTION B - Proceed to Building Block 4 (Question Generation):
If ready to begin generating questions directly from blueprint

OPTION C - Pause for Review:
If additional stakeholder review needed before proceeding

Which path forward?"
```

---

## OUTPUT STRUCTURE: ASSESSMENT BLUEPRINT DOCUMENT

### Complete Blueprint Template

Building Block 2 produces a comprehensive Assessment Blueprint document following standardized structure:

```markdown
# Assessment Blueprint: [Topic] ([Course Code])

**Document Status:** Finalized  
**Version:** 1.0  
**Date:** [Creation Date]  
**Framework:** Modular Question Generation System - Building Block 2  
**Prepared By:** [Teacher Name]  
**Approved:** [Approval Date]

---

## EXECUTIVE SUMMARY

**Assessment Type:** [Formative / Summative / Hybrid]  
**Purpose:** [Brief description of assessment purpose and usage]

**Scope:**
- Total Questions: [N]
- Total Points: [P]
- Estimated Time: [T] minutes
- Learning Objectives: [M] across [X] Bloom's levels

**Assessment Character:** [Accessible/Balanced/Challenging] assessment with 
[formative/summative] emphasis and [good/excellent] question variety

**Target Population:** [Course level, student characteristics]

---

## SECTION 1: ASSESSMENT CONFIGURATION

### Purpose and Context

**Primary Purpose:** [Detailed description]

**Usage Context:**
- When: [Timing in course]
- Stakes: [Grade weight / impact]
- Preparation: [Expected student preparation]

### Technical Configuration

**Platform:** [LMS name and version]

**Feedback Configuration:**
- Timing: [Immediate / Deferred / None]
- Detail: [Correct answer / Explanation / Hints]
- Attempt policy: [Unlimited / Limited to N / Single attempt]

**Time Configuration:**
- Recommended time: [T] minutes
- Enforced time limit: [Yes with T minutes / No]
- Time buffer: [X]% for students needing accommodations

**Grading Configuration:**
- Automated grading: [X]% of questions
- Manual grading: [Y]% of questions
- Point scale: [P] total points
- Passing threshold: [if applicable]

### Constraints and Limitations

**Documented Constraints:**
- Available time: [constraint]
- Grading resources: [constraint]
- Platform capabilities: [constraint]
- Student population: [constraint]

---

## SECTION 2: LEARNING OBJECTIVES

### Learning Objectives by Cognitive Level

Total Objectives: [M]

#### REMEMBER ([N1] objectives, [X1]%)

**LO1:** [Identifier]  
**Description:** [Complete objective statement]  
**Content Tier:** [1/2/3 from Building Block 1]  
**Questions Allocated:** [N] questions  

**LO2:** [Continue for all Remember objectives]

#### UNDERSTAND ([N2] objectives, [X2]%)

[Same structure for all Understand objectives]

#### APPLY ([N3] objectives, [X3]%)

[Same structure for all Apply objectives]

#### ANALYZE ([N4] objectives, [X4]%)

[Same structure for all Analyze objectives]

#### EVALUATE ([N5] objectives, [X5]%)

[Same structure for all Evaluate objectives]

### Learning Objective Coverage Summary

| Objective | Bloom's | Tier | Questions | Points | Coverage |
|-----------|---------|------|-----------|--------|----------|
| LO1       | Remember| 1    | [N]       | [P]    | [Status] |
| LO2       | Remember| 1    | [N]       | [P]    | [Status] |
[Continue for all objectives]

**Coverage Status Key:**
- Adequate: 4+ questions
- Sufficient: 3 questions  
- Minimum: 2 questions
- Under-covered: <2 questions (requires attention)

---

## SECTION 3: DISTRIBUTION STRATEGY

### Bloom's Level Distribution

**Overall Cognitive Profile:**
- Foundational (Remember + Understand): [X]%
- Higher-Order (Apply + Analyze + Evaluate): [Y]%

| Bloom's Level | Questions | % of Total | Points | Rationale |
|---------------|-----------|------------|--------|-----------|
| Remember      | [N1]      | [X1]%      | [P1]   | [Brief rationale] |
| Understand    | [N2]      | [X2]%      | [P2]   | [Brief rationale] |
| Apply         | [N3]      | [X3]%      | [P3]   | [Brief rationale] |
| Analyze       | [N4]      | [X4]%      | [P4]   | [Brief rationale] |
| Evaluate      | [N5]      | [X5]%      | [P5]   | [Brief rationale] |

**Distribution Rationale:**  
[Paragraph explaining overall distribution logic]

### Question Type Distribution

| Question Type | Questions | % of Total | Auto-Grade | Purpose |
|---------------|-----------|------------|------------|---------|
| MC-Single     | [N1]      | [X1]%      | Yes        | [Purpose] |
| MC-Multiple   | [N2]      | [X2]%      | Yes        | [Purpose] |
| True/False    | [N3]      | [X3]%      | Yes        | [Purpose] |
| Fill-Blank    | [N4]      | [X4]%      | Yes        | [Purpose] |
| Matching      | [N5]      | [X5]%      | Yes        | [Purpose] |
| Text Area     | [N6]      | [X6]%      | No         | [Purpose] |
| Extended Text | [N7]      | [X7]%      | No         | [Purpose] |
[Include interactive formats if applicable]

**Auto-Graded vs. Manual-Graded:**
- Auto-graded: [X]% ([N] questions)
- Manual-graded: [Y]% ([N] questions)

**Grading Workload Estimate:**  
[Estimated time for manual grading based on question count and type]

### Difficulty Distribution

| Difficulty | Questions | % of Total | Expected Success Rate |
|------------|-----------|------------|-----------------------|
| Easy       | [N1]      | [X1]%      | 70-90%               |
| Medium     | [N2]      | [X2]%      | 40-70%               |
| Hard       | [N3]      | [X3]%      | 20-40%               |

**Difficulty Profile:**  
[Description: Accessible/Balanced/Challenging]

**Difficulty Assignment Approach:**  
[Distributed / Progressive]

#### Difficulty Distribution by Cognitive Level

| Bloom's   | Easy | Medium | Hard | Total |
|-----------|------|--------|------|-------|
| Remember  | [N]  | [N]    | [N]  | [N]   |
| Understand| [N]  | [N]    | [N]  | [N]   |
| Apply     | [N]  | [N]    | [N]  | [N]   |
| Analyze   | [N]  | [N]    | [N]  | [N]   |
| Evaluate  | [N]  | [N]    | [N]  | [N]   |

---

## SECTION 4: QUESTION SPECIFICATION TABLE

**Blueprint Table Structure:**

| Q# | LO | Bloom's | Type | Difficulty | Points | Content Focus |
|----|----|---------|----|------------|--------|---------------|

**Complete Question Specifications:**

| Q# | LO | Bloom's | Type | Difficulty | Points | Content Focus |
|----|----|---------|----|------------|--------|---------------|
| Q1 | LO1 | Remember | Fill-blank | Easy | 1 | [Specific content element] |
| Q2 | LO1 | Remember | MC-Single | Easy | 1 | [Specific content element] |
| Q3 | LO1 | Remember | MC-Single | Medium | 1 | [Specific content element] |
| Q4 | LO2 | Remember | Matching | Easy | 2 | [Specific content element] |
| Q5 | LO3 | Understand | MC-Single | Easy | 2 | [Specific content element] |
| Q6 | LO3 | Understand | MC-Single | Medium | 2 | [Specific content element] |
| Q7 | LO3 | Understand | Text Area | Medium | 3 | [Specific content element] |
[Continue for all [N] questions]

**Table Legend:**
- Q#: Question number (sequencing may be adjusted during generation)
- LO: Learning Objective identifier from Section 2
- Bloom's: Cognitive level
- Type: Question format code
- Difficulty: Easy / Medium / Hard
- Points: Point value for question
- Content Focus: Specific content element or scenario

---

## SECTION 5: COVERAGE VALIDATION

### Coverage Matrix

**Questions per Learning Objective:**

| Learning Objective | Planned Questions | Points | Status |
|--------------------|-------------------|--------|--------|
| LO1 (Remember)     | [N]               | [P]    | ‚úÖ Adequate |
| LO2 (Remember)     | [N]               | [P]    | ‚úÖ Adequate |
| LO3 (Understand)   | [N]               | [P]    | ‚úÖ Adequate |
[Continue for all objectives]

**Coverage Standards:**
- ‚úÖ Adequate: 4+ questions
- ‚úÖ Sufficient: 3 questions
- ‚ö†Ô∏è  Minimum: 2 questions  
- ‚ùå Under-covered: <2 questions (requires adjustment)

### Content Tier Coverage

**Tier 1 (Essential Content) Coverage:**
- Objectives: [N] of [Total Tier 1]
- Questions: [N] questions ([X]% of assessment)
- Status: ‚úÖ Comprehensive coverage

**Tier 2 (Important Content) Coverage:**
- Objectives: [N] of [Total Tier 2]
- Questions: [N] questions ([X]% of assessment)
- Status: ‚úÖ Adequate coverage

**Tier 3 (Enrichment Content) Coverage:**
- Objectives: [N] of [Total Tier 3]
- Questions: [N] questions ([X]% of assessment)
- Status: [Minimal / None] coverage (as intended)

### Validation Summary

**Coverage Validation Results:**

‚úÖ All learning objectives included  
‚úÖ All objectives meet minimum coverage (2+ questions)  
‚úÖ Essential content (Tier 1) comprehensively covered  
‚úÖ Important content (Tier 2) adequately covered  
‚úÖ No critical coverage gaps identified

---

## SECTION 6: QUALITY ASSURANCE CHECKLIST

### Blueprint Quality Standards

**Constructive Alignment:**
- ‚úÖ Questions align with learning objectives
- ‚úÖ Question types appropriate for cognitive levels
- ‚úÖ Difficulty levels reasonable for content
- ‚úÖ Assessment purpose matches configuration

**Distribution Balance:**
- ‚úÖ Bloom's distribution matches learning priorities
- ‚úÖ Question type variety adequate
- ‚úÖ Difficulty distribution appropriate for purpose
- ‚úÖ No excessive concentration in single category

**Technical Feasibility:**
- ‚úÖ All question types supported by platform
- ‚úÖ Assessment time reasonable for scope
- ‚úÖ Grading workload manageable
- ‚úÖ Point allocation clear and defensible

**Coverage Adequacy:**
- ‚úÖ All objectives assessed
- ‚úÖ Essential content emphasized
- ‚úÖ No systematic gaps
- ‚úÖ Minimum questions per objective met

---

## SECTION 7: IMPLEMENTATION NOTES

### For Question Generation (Building Block 4)

**Blueprint Usage:**
- Question Specification Table (Section 4) provides complete specifications
- Content Focus column indicates specific scenarios/contexts
- Point values and difficulty levels must be maintained
- Question types and cognitive levels are required specifications

**Flexibility Boundaries:**
- Specific wording: Flexible within specifications
- Content examples: Flexible within focus area
- Question sequencing: Flexible (optimize during generation)
- Specification changes: Require formal blueprint revision

### For Quality Assurance (Building Block 5)

**Validation Priorities:**
1. Verify alignment with blueprint specifications
2. Confirm cognitive levels match claimed levels
3. Check difficulty calibration against definitions
4. Validate point allocation consistency

### For Stakeholders

**Blueprint Functions:**
- Provides transparent assessment plan
- Documents pedagogical decisions and rationale
- Establishes quality standards
- Guides iterative improvement

---

## APPROVAL AND VERSION CONTROL

**Blueprint Finalized:** [Date]  
**Approved By:** [Teacher Name]  
**Framework Version:** Modular QGen v3.0  
**Building Block:** BB2 - Assessment Design

**Change Control:**  
Significant changes require blueprint revision and new version number.  
Minor adjustments (e.g., question sequencing) do not require revision.

**Next Phase:** Building Block 3 (Technical Setup) or Building Block 4 (Question Generation)

---

**Document Status:** Finalized Assessment Blueprint  
**Ready for:** Question Generation Phase
```

---

## TRANSITION PATHWAYS FROM BUILDING BLOCK 2

### Standard Path: Proceed to Building Block 3

Most implementations proceed from Building Block 2 (Assessment Design) to Building Block 3 (Technical Setup) before question generation:

```
Building Block 2 Complete ‚Üí Building Block 3 (Technical Setup)

Rationale for Building Block 3 first:
- Establishes technical metadata requirements
- Configures platform-specific settings
- Creates question type templates
- Validates technical feasibility before generation

Building Block 3 outputs (metadata configuration, technical specifications) 
inform question generation in Building Block 4.

This sequential path recommended for:
- First-time use of framework
- Platform-specific requirements (Inspera, Canvas, etc.)
- Assessments using complex question types
- When technical validation needed before generation work begins
```

### Alternative Path: Skip to Building Block 4

Some implementations skip directly from Building Block 2 to Building Block 4 (Question Generation):

```
Building Block 2 Complete ‚Üí Building Block 4 (Question Generation)

Rationale for skipping Building Block 3:
- Technical requirements already known
- Using standard LMS with familiar setup
- Simple question types not requiring technical specification
- Metadata can be added during/after generation

This path recommended for:
- Experienced users familiar with technical requirements
- Straightforward assessments using standard formats
- Rapid development when technical aspects understood
- Iterative development with technical refinement later
```

### Revision Path: Return to Building Block 1

Some blueprint review processes identify need to revisit content analysis:

```
Building Block 2 Review ‚Üí Building Block 1 (Content Analysis Revision)

Reasons for returning to Building Block 1:
- Coverage gaps reveal missing content analysis
- Learning objectives misaligned with instruction
- Content priorities different than originally understood
- New instructional materials available

After Building Block 1 revision, return to Building Block 2 to update blueprint 
based on revised content analysis.
```

The AI system presents these pathways explicitly after blueprint approval, allowing teachers to select the path matching their specific situation and experience level.

---

## CRITICAL OPERATIONAL NOTES

### For AI Systems Using Building Block 2

**Dialogue Management:**
- Propose recommendations based on frameworks, never dictate
- Present rationale for all recommendations using pedagogical principles
- Facilitate teacher decision-making rather than substituting for it
- Validate choices against pedagogical standards while respecting teacher expertise
- Document all decisions explicitly for transparency

**Decision Authority:**
- Teacher makes all pedagogical decisions (assessment type, distributions, emphasis)
- AI system makes no autonomous judgments about "correct" distributions
- Framework provides guidance, teacher determines appropriateness for context
- Validate technical feasibility, not pedagogical validity

**Calculation Accuracy:**
- All percentage calculations must sum to 100% with rounding clearly explained
- Question counts must sum to specified total
- Check coverage calculations ensure all objectives receive minimum questions
- Time estimates should include buffer for slower students

**Validation Rigor:**
- Run all validation checks before presenting blueprint
- Flag warnings even if proceeding is possible
- Never suppress validation failures
- Present implications of decisions clearly

### For Teachers Using Building Block 2

**Key Decision Points:**
Stages 2, 4, 5, and 6 require explicit teacher decisions. These decisions cascade through subsequent stages, so consider implications carefully:

- **Assessment type** (Stage 2) influences all subsequent distribution recommendations
- **Bloom's distribution** (Stage 4) determines cognitive character of assessment
- **Question type mix** (Stage 5) determines grading workload and assessment depth
- **Difficulty distribution** (Stage 6) determines accessibility and discrimination

**Blueprint as Contract:**
Once finalized, the blueprint serves as authoritative specification for question generation. Significant changes require formal revision process to maintain systematic integrity.

**Iteration Expectation:**
First blueprint rarely perfect. Expect refinement after review, particularly:
- Coverage adjustments when gaps identified
- Type mix adjustments when grading implications considered
- Difficulty adjustments when student readiness reassessed

**Quality Standards:**
Maintain focus on constructive alignment throughout:
- Questions must genuinely assess claimed objectives
- Question types must suit cognitive levels
- Difficulty must calibrate appropriately for student level
- Distributions must serve stated assessment purpose

---

## SUMMARY

Building Block 2 transforms validated learning objectives from Building Block 1 into comprehensive assessment blueprints through systematic seven-stage dialogue process. The building block balances pedagogical decision-making (teacher-driven) with systematic frameworks (AI-supported) to produce detailed specifications that guide question generation while ensuring constructive alignment, adequate coverage, and technical feasibility.

The hybrid nature of this building block‚Äîrequiring both teacher judgment and systematic structure‚Äîdistinguishes it from purely procedural components. Success in Building Block 2 depends on productive dialogue where teachers articulate pedagogical priorities while the AI system provides analysis, recommendations, and validation based on assessment design principles.

The Assessment Blueprint produced in this building block serves multiple functions: operational guide for question generation, validation tool for coverage and alignment, communication artifact for stakeholders, and reference document during quality assurance. Creating comprehensive blueprints before question generation prevents common problems of gaps, misalignments, and imbalances discovered too late for efficient correction.

Building Block 2 connects directly to Building Block 1 (providing learning objectives foundation) and flows into either Building Block 3 (technical setup) or Building Block 4 (question generation), depending on implementation context and technical requirements. This flexible workflow structure allows the Modular QGen framework to adapt to different starting contexts while maintaining systematic rigor throughout the process.

---

**Document Status:** Comprehensive Building Block Documentation  
**Version:** 2.0 (Revised following Academic Writing Guide)  
**Component Type:** Building Block 2 (formerly Component 2)  
**Framework:** Modular Question Generation System  
**Revision Date:** [Current Date]  
**Revision Approach:** Academic prose with theoretical grounding, minimal bullet lists, comprehensive process documentation
